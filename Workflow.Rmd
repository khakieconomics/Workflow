---
title: "A Modern Statistical Workflow"
author: "Jim Savage"
date: "9 April 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F)
knitr::knit_hooks$set(inline = as.character)
library(BSEcon); library(ggthemes); library(latex2exp)
rinline <- function(code){
  html <- '<code  class="r">``` CODE` ```</code>'
  sub("CODE", code, html)
}
```

## Why think about workflow?

Most of this book is about implementing common econometric models using Bayesian techniques. 
These work-horse models are really just the starting point for the sorts of models you might
start to implement. As you become more creative in your model design, it will help to have 
a rigorous workflow. Focusing on this workflow will help you build better understanding of
what is going on in your complex models, as well as catching bugs or improving the computational
efficiency in the implementation. 

The workflow is fairly simple: 

1. Prepare and visualize your data
2. Come up with a generative model for the data. The first time, it should be as simple as possible
3. Choose some priors for the model's unknowns. Draw some values from these priors
4. Simulate some "fake data" from your model given a draw from these priors
5. Fit your model to the fake data and check for the quality of the fit
6. Check that you were able to capture these "known unknowns". Possibly repeat 4-6, to get an understanding of bias or other issues in your fitting procedure
7. Fit the model to your real data, check the fit
8. Argue about the results with your friends and colleagues
9. Go back to 2. with a slightly richer model. Repeat. 
10. Think carefully about what decisions will be made from the analysis, encode a loss function, and perform statistical decision analysis. 

Each of these steps can be somewhat involved. This chapter described each step in some detail, with worked 
examples. Where more depth may be required, we encourage you to read the referenced resources. 
We start by providing some background on basic data manipulation and plotting in R. Though this is not 
a book about these "core data science" skills, the functions described here are used throughout the book, and 
knowing them will be necessary for the exercises throught the book. Next we cover the concept of a model, random variables, 
their distributions, the likelihood function, priors, and Bayes' rule. These concepts form the basis of 
building probabilitic generative models. Next, we focus on how to choose priors, and why this depends on your model
and your data. Then we go through the workflow above in details. Finally, we work through the 
full workflow for a basic credit scoring exercise. 

\newpage

## Part 1: Some basic data munging using the tidyverse

This chapter deals with building probabilistic models of our data. Building such models requires
that you understand at least the basics of probability, including how we think about random variables
and their distributions, and a collection of commonly-used parametric distributions. It also requires that you 
can handle, manipulate and visualize data, which we'll call "core data science skills". This section provides 
a brief introduction to this required knowledge at a level that will help you understand what's going on 
later, but not much deeper. It is certainly worth investing in improving your understanding
of core data science skills and probability theory, but we leave a lot of the details to some other 
resources. For some wonderful recent resources on data science and probability theory, 
we recommend Garrett Grolemund and Hadley Wickham's R for Data Science, available at 
[http://r4ds.had.co.nz/](http://r4ds.had.co.nz/), and Michael Betancourt's introduction 
to probability, available at 
[https://betanalpha.github.io/assets/case_studies/probability_theory.html](https://betanalpha.github.io/assets/case_studies/probability_theory.html).

Preparing and visualizing your data should be the first step to your analysis. It will help you discover potential issues
far sooner in the analysis, and will help you to spot relationships that ought to be modeled. For data visualization 
in R, we strongly recommend you become acquainted with the `ggplot2` package used within the `tidyverse` family of packages. 
These allow for a swift exploration of your data along various facets. The `tidyverse` family of packages do have a 
relatively steep learning curve, particularly for those experienced in Excel or Stata. Yet they are certainly worth investing in. 

The authoritative resource for this part of the workflow is the Data Science in R book mentioned above. It is available online
for free, and is coauthored by the primary developer of the `tidyverse` family of packages. This section describes the main 
functions within this package which we'll use throughout the book. 

**Tidy data**

One data format that makes both data preparation and modeling easier is the so-called "tidy data" format. The idea
is to use data for which the following hold

- Each variable is a column
- Each observation is a row

This is often _not_ how data is provided to economists. For example, if you download the state accounts 
for Australia you will have data that look something like the following: 

```{r}
library(tidyverse); library(readxl)
state_accounts <- read_excel("data/state_accounts.xlsx")

state_accounts %>% select(Date:QLD) %>% filter(1:n()<6) %>% add_dots() %>% knitr::kable()
```

You will see here that each column corresponds to a state, and each entry corresponds to the state-year pair. 

Or take the default output from the World Bank's data portal. Here, GDP per capita: 

```{r}
GDP_pc <- read_csv("data/API_NY.GDP.PCAP.CD_DS2_en_csv_v2.csv", skip = 4)

GDP_pc %>% select(`Country Name`, `2012`:`2015`) %>% group_by(`Country Name`) %>%
  mutate_all(funs(round)) %>% ungroup %>% filter((1:n())<6) %>% add_dots %>% knitr::kable(digits = 1)
```

Here we see that each row corresponds to a country, each column to a year, and each entry to a country-year pair. 

What does a tidy data representation of these datasets look like? First let's take the state accounts. In order to 
perform the following operations, you should have already installed the `tidyverse` and `readxl` packages in R. You can do this by running 

```
install.packages(c("tidyverse", "readxl"))
```

To convert these datasets into a tidy form, we'll use the `gather` function. Let's run it, to see what 
the output looks like, then explain the function. 

```{r, echo = T}
library(tidyverse); library(readxl)
# read data
state_accounts <- read_excel("data/state_accounts.xlsx")[1:26,]

# "Gather" into long or tidy form
state_accounts_tidy <- gather(state_accounts, State, `Gross state product`, -Date)
```

Which gives us 

```{r}
nsw <- state_accounts_tidy %>% filter(1:n()<6) %>% add_dots(type = "rows") %>% as_data_frame()

vic <- state_accounts_tidy %>% filter(State == "ACT") %>% filter(1:n() < 6) %>% add_dots(type = "rows") %>% as_data_frame()

bind_rows(nsw, vic) %>% knitr::kable()
```

So what did we just do? The `gather` function takes the following arguments

```
gather(data, key, value, ...)
```

The `data` argument is the data frame we wish to convert. In our case, it is the wide/non-tidy gross state product. 
The `key` argument is the name we want to give to the new column that will be formed by the values which were
column names in the wide dataset. The old dataset has abbreviated Australian state names, so we'll use `State` as the 
key. The `value` argument is the name we want to give to the new column formed by the values which will be "stacked"
in the tidy format. Here, we want it to say `Gross state product`. Finally, the `...` argument lets us select the 
columns we wish to stack together. By default, all will be used. Any columns which we don't want to use as keys
we should exclude. We do that using `-Date`. Run it again without excluding that column and see what happens. 
We can convert from "tidy" data to a wide format using the `spread()` function, for which we provide the key and value
columns which will be converted into column headers and the values within each column. 

Let's repeat the same exercise for the World Bank data. 

\bigskip

```{r, echo = T}
# There are four meaningless rows at the top of the file, which we exclude
gdp_pc <- read_csv("data/API_NY.GDP.PCAP.CD_DS2_en_csv_v2.csv", skip = 4) 

# Convert to tidy format and arrange by country and year
gdp_pc_tidy <- gather(gdp_pc, Year, `GDP per capita`, 
                      -`Country Name`:-`Indicator Code`) %>% 
  arrange(`Country Name`, Year)
```

Note three new things we've done here. The first is that we've excluded several columns. Because
in the raw data these columns are adjacent, we use the notation \texttt{-\`{}Country Name\`{}:-\`{}Indicator Code\`{}}
to "de-select" columns with names \texttt{\`{}Country Name\`} through \texttt{\`{}Indicator Code\`{}}. We've also
used the "pipe" notation `%>%`, which takes the output from the preceding function and "pipes" it into the first
argument of whatever function comes next. Finally, we've used the `arrange()` function, which sorts the resulting
data frame by whatever columns we specify; in this case, the country name and year columns. 



### The split-apply-combine idea

**What was the point of putting data in this tidy format?**

We almost always want to deal with data in this "tidy" format. The reason is that it allows us to use 
many extremely powerful data manipulation and visualization methods. These are primarily based on the 
"split-apply-combine idea", in which we can **split** our dataset up into sub-groups defined by one or more columns, 
**apply** a function to each subgroup, and combine the result into a new data frame. The package `dplyr` 
within the `tidyverse` family of packages provides support for operations of this flavour. Let's look at two: 
`mutate` and `summarise`. 

`mutate` is one of a few functions we use for the "apply" part of "split-apply-combine"; it simply 
adds a new column to the data frame based on existing columns and, possibly, the grouping. 
For instance, in our example we might want to convert State Domestic Product into an index that is 1 in the year
2000, to allow for comparisons in State growth using that point in time as the base. We'd do that like so: 

```{r, echo = T}
state_accounts_index <- state_accounts_tidy %>% 
  mutate(Date = as.Date(Date)) %>%
  group_by(State) %>%
  mutate(`GSP in 2000` = `Gross state product`[Date=="2000-06-30"],
         Index = `Gross state product`/`GSP in 2000`) %>%
  group_by(Date) %>% 
  mutate(`Weighted mean index` = weighted.mean(Index, `Gross state product`)) %>%
  ungroup
```

Here, we've added two new columns, the first, `GSP in 2000` to show how we can pull out a single value of 
the `Gross state product` column using a logical test, and a second, `Index`, to show that we can use 
previously-defined columns in defining a new one. Note that in order to use the logical test in the second
`mutate` call, we need to first convert `Date` to a `Date` class. 

What did `group_by(State)` do? In the "split-apply-combine" mantra, it is the "split"---we separate the data so that
any functions used after the `group_by()` call _only refer to observations in that group_. To further illustrate
this, we've performed two groupings---the first by `State`, the second by `Date`. You can also group
by multiple columns at the same time. 

If `mutate` allows us to add columns while refering to variables only within a group, then how can we 
perform group summaries? This is where the `summarise()` (or `summarize()` if you prefer) function comes in. 
`summarise()` takes a grouped data frame, and allows you to specify "summaries"---values of length 1---for each
group. For example, say each state has GSP $Y_{t}$ in period $t$ and it grows at a constant annual growth rate of $g$
per year. Let $Y_{0}$ be the initial GSP. Then

$$
Y_{t} = Y_{0}(1 + g)^{t}
$$
Rearranging this gives us the average growth rate between periods $0$ and $t$ as 

$$
g = \left(\frac{Y_{t}}{Y_{0}}\right)^{\frac{1}{t}} - 1
$$
Let $s$ by the standard deviation of the actual annual growth rates around this "constant" rate. Let's produce
summaries that describe these two statistics for each state, using the tidy data 

```{r, echo = T}
state_growth <- state_accounts_tidy %>% 
  group_by(State) %>%
  mutate(`Annual growth %` = `Gross state product`/lag(`Gross state product`)) %>% 
  summarise(`Constant growth rate %` = 100*((last(`Gross state product`)/
                                               first(`Gross state product`))^(1/n()) - 1),
            `SD growth %` = 100*sd(`Annual growth %` - `Constant growth rate %`, na.rm = T)) 
```

Which gives

```{r}
state_growth%>%
  knitr::kable(digits = 2)
```

What have we done here? We've "grouped by" `State` (essentially splitting the tidy data frame into data frames for 
each state), then "summarised", reducing each state's data frame to a single row, where each column is defined
within the `summarise()` function. The "combine" step of "split-apply-combine" happens automatically, after `summarise()` or `mutate()` is called. 

Note that we've used a few functions that come packaged within the `dplyr`/`tidyverse` packages: `lag()`, `last()` and 
`first()`. `lag()` (and its counterpart, `lead`) returns the previous observation within a group; you can also ask
for further lags by specifying the lag order `n`, `lag(x, n)`. The `last()` and `first()` functions return the last
and first observations respectively. There is another function `nth()` which allows you to specify the position of the 
element you want to return---so `nth(c(1,3,5,7), 3) = 5`. 

\newpage

## Part 2: Visualizing your data

> The most powerful econometric tool is the eye

Once our data are in a tidy format we can use the `ggplot2` package---also included in the `tidyverse` family, 
to explore our data. This section provides a very brief introduction to this package. First we build a generic plot, 
to show how we can make attractive, publication-ready plots. Next we perform some exploratory data analsysis 
on a large number of US consumer loans (we'll use this dataset throughout the chapter). 

The central idea in `ggplot2` is to build up a plot, element by element. The basis of a ggplot object is 
a blank plot, created with `ggplot()`. On top of this, we can add more layers, each a "geom". For instance, 
a histogram is a type of geom; a set of lines is another; a density plot another still. The aesthetics of the geom, 
for instance the position, colour, size etc., are given by a mapping from variables (columns in your tidy data) to 
"aesthetics". There are several new terms there---let's illustrate with some examples. 

We can start with a blank plot, given by piping our tidy data into ggplot. As you can see, if we don't add any geoms, 
we don't have a very interesting plot. 

```{r, echo = T}
state_accounts_index %>% 
  ggplot()
```

We might want to look at what has happened to our index of gross state product. To do that, we can add `geom_line()`, 
which has two mandatory aesthetics (the x and y mapping) and some optional ones, like group, colour (for when you have a column that defines which lines correspond to different colours), size, etc. For example: 

```{r, echo = T}
state_accounts_index %>% 
  ggplot() +
  geom_line(aes(x = Date, y = Index, colour = State))
```

This does a good job at illustrating the fact that Western Australia grew extremely quickly during the 
mining boom, and NSW and Tasmania did not. Yet the economies of NSW and Victoria are far larger than other
states. How might we illustrate this? One approach is to create more mappings between the variables in our data
and the acceptable aesthetics, in this case, points whose size and opacity is related to the size of those states' economies. 

```{r, echo = T}
state_accounts_index %>% 
  ggplot() +
  geom_line(aes(x = Date, y = Index, 
                colour = State, alpha  = `Gross state product`)) +
  geom_point(aes(x = Date, y = Index, 
                 colour = State, size = `Gross state product`, 
                 alpha = `Gross state product`))
```

Now we can see that the two biggest states have grown fairly slowly relative to resource rich 
Western Australia and Queensland. The Northern Territory barely figures, given how few people live there. 

This plot is starting to look ok. We still need to add some trimmings. Let's use a more attractive theme 
(The `theme_bw()` theme maybe), and give it some new labels. Also, the alpha/size 
legend (a `guide`) isn't very helpful, so we will remove it. 

```{r, echo = T}
state_accounts_index %>% 
  ggplot() +
  geom_path(aes(x = Date, y = Index, 
                colour = State, 
                size = `Gross state product`), lineend = "round") +
  theme_bw() +
  labs(x = "Year",
       y = "Proportion of 2000 GSP",
       title = "Gross State Product relative to 2000",
       subtitle = "Size/shading proportional to GSP",
       caption = "Chained real GSP. Source: ABS") +
  guides(size = F, alpha = F)
```

Now let's say we take this to a meeting and we receive feedback that the size/shading of the points
wasn't really a very good way of illustrating the difference in economic activity across the states. 
instead, they want to see raw GSP, but have the chart split in two (one for big states, one for small ones). 
We can do something like this very easily: 

```{r, echo = T}
state_accounts_index %>% 
  mutate(`State type` = if_else(State %in% c("NSW", "WA", "QLD", "VIC"), 
                                "Large states", "Small states")) %>%
  ggplot() +
  geom_line(aes(x = Date, y = `Gross state product`, 
                colour = State)) +
  theme_bw() +
  labs(x = "Year",
       y = "Millions of 2015 dollars",
       title = "Gross State Product",
       caption = "Chained real GSP. Source: ABS") +
  guides(size = F, alpha = F) +
  facet_grid(`State type` ~., scales = "free_y") 
```

What have we done here? We added two new things: the first mutates our source data to add a new column, 
which is `"Large states"` for the bigger states and `"Small states"` otherwise. We then use `facet_grid()`
to split the plot into two smaller plots, each with their own y axis. 

There are still a couple of slightly annoying thing about the plots above. The legend does not line up 
very nicely with the order of the lines on the plot. This is because by default, the discrete colour
variable is converted into a factor where the order is given alphabetically. We want it to be ordered by 
the final GSP. So let's do this. Also, the scientific notation on the y-axis has to go. So let's rescale in 
terms of billions of dollars. 

```{r, echo = T}
state_accounts_index %>% 
  # Get the last GSP for each state
  group_by(State) %>% 
  mutate(final_gsp = last(`Gross state product`)) %>% 
  ungroup %>% 
  # Reorder the State variable by decreasing order of final GSP
  mutate(State = reorder(State, -final_gsp),
         `State type` = if_else(State %in% c("NSW", "WA", "QLD", "VIC"), 
                                "Large states", "Small states")) %>%
  ggplot() +
  geom_line(aes(x = Date, y = `Gross state product`/1000, 
                colour = State)) +
  theme_bw() +
  labs(x = "Year",
       y = "Billions of 2015 dollars",
       title = "Gross State Product",
       caption = "Chained real GSP. Source: ABS") +
  guides(size = F, alpha = F) +
  facet_wrap(~`State type`, scales = "free_y") 
```

We've just taken a look at some useful techniques to manipulate and visualize data. Now let's 
use these to perform an exploratory analysis of a real-life commercial dataset. 

\newpage

### A worked example of data preparation and visualization

Now we know the basics of loading data and producing produce production-ready plots. Let's see 
how we might want to use these tools for an exploratory data anlalysis of some real data 
from a non-bank lender in the US. Later in the chapter we'll use this dataset to build 
our first models. We will use a dataset of all loans made by US lending club platfom
Lending Club, from 2007--2011. You can download a zip file containing these loans from [https://www.lendingclub.com/info/download-data.action](https://www.lendingclub.com/info/download-data.action). 

Before starting, what are we trying to learn? Some important questions when building any model will include: 

- What is it that we're interested in? 
- What *type* of data are we modeling? (Continuous, binary, categorical etc.)
- What relationships do we see between the variables in the data? 
- Are there outliers, missing values, structural breaks that we should think about? 

If we're looking at loans data, then it's probable that the question is one of "underwriting"---deciding
whether or not to give a borrower a loan based on the information they provide when they apply. That is, 
we're probably interested in whether a loan defaults---is charged off---and if so, how large the loss is. 

First let's read the data and take a quick look.

```{r, echo = T}
library(tidyverse)
loans <- read_csv("data/LoanStats3a.csv", skip = 1)
head(loans[,1:10])
```

There are obviously a huge number of variables in this dataset, so let's look 
at a few variables that might be relevant. 

```{r, echo = T}
# Make a data_frame that contains some relevant variables
mini_loans <- loans %>% 
  select(loan_status, total_pymnt, loan_amnt, term, int_rate, 
         grade,  emp_length,  addr_state, mths_since_last_delinq,
         home_ownership, revol_bal, revol_util, annual_inc,
         purpose)

glimpse(mini_loans)
```

What do we see? First, the data types are all quite different. The percentages are recorded as characters, 
while they should be numbers. So is the loan `term`. We probably want to encode the `loan_status` as a numeric, 
for reasons that will soon become clear. And the missing values in `mths_since_last_delinq` are meaningful, as 
they correspond to a borrower not having had any delinquencies. 

Let's clean up the data to do some exploratory analysis. 

```{r, echo = T}
library(readr)
# First take a look at the unique loan_status types and their counts
table(mini_loans$loan_status)

# Now make some new variables/clean existing ones
mini_loans <- mini_loans %>% 
  mutate(defaulted = ifelse(grepl("Charged Off", loan_status), 1, 0),
         term = parse_number(term),
         int_rate = parse_number(int_rate),
         revol_util = parse_number(revol_util),
         no_delinquencies = as.numeric(is.na(mths_since_last_delinq)))
```

What did we ust do? First, we created a new variable `defaulted`, which is 1 when the `loan_status` variable contains 
the words "Charged Off", and 0 otherwise. The reason we turn it into a numeric is that we can now calculate
the proportion of loans in a group that have defaulted by simply taking the average of `defaulted` within the group. 
For example, if we want to see which loan purposes are associated with more defaults, we could do the following: 

\newpage 

```{r, echo = T}
mini_loans %>% 
  # Get rid of the observations with missing values for purpose
  filter(!is.na(purpose)) %>%
  # calculate groupwise proportions and their standard errors
  group_by(purpose) %>% 
  summarise(proportion_defaulted = mean(defaulted),
            N = n(),
            standard_error = sqrt(proportion_defaulted * (1 - proportion_defaulted)/N)) %>% 
  # Use the reordering trick to make the plot attractive
  mutate(purpose = reorder(purpose, proportion_defaulted)) %>%
  # Put the plot together
  ggplot(aes(x = purpose)) +
  # Roughly 95% confidence intervals for the proportions
  geom_linerange(aes(ymin = proportion_defaulted - 2*standard_error, 
                     ymax = proportion_defaulted + 2*standard_error), 
                 colour = "green") +
  geom_point(aes(y = proportion_defaulted)) +
  theme_bw() +
  coord_flip() +
  labs(title = "Default rates by loan purpose")
```

\newpage
Lending Club use their own internal credit-worthiness `grade`, which should relate to 
default rates. Let's take a look.


```{r, echo = T}
mini_loans %>% 
  filter(!is.na(grade)) %>%
  # calculate groupwise proportions and their standard errors
  group_by(grade) %>% 
  summarise(proportion_defaulted = mean(defaulted),
            N = n(),
            standard_error = sqrt(proportion_defaulted * (1 - proportion_defaulted)/N)) %>% 
  # Put the plot together
  ggplot(aes(x = grade)) +
  # Roughly 95% confidence intervals for the proportions
  geom_linerange(aes(ymin = proportion_defaulted - 2*standard_error, 
                     ymax = proportion_defaulted + 2*standard_error), 
                 colour = "green") +
  geom_point(aes(y = proportion_defaulted)) +
  theme_bw() +
  labs(title = "Default rates by loan Lending Club grade")
```

This indeed confirms that Lending Club's credit scoring methodology does a good job 
at distinguishing loans that are likely to default and those that are not. One question that
we might have at this point is whether the loan purpose appears to predict defaults
after we account for the grade. We can take a look by faceting.

\newpage

```{r, echo = T}
mini_loans %>% 
  filter(!is.na(grade) & !is.na(purpose)) %>%
  # calculate groupwise proportions and their standard errors
  group_by(grade, purpose) %>% 
  summarise(proportion_defaulted = mean(defaulted),
            N = n()) %>% 
  group_by(purpose) %>% 
  mutate(average_default_rate = mean(proportion_defaulted)) %>%
  ungroup %>% 
  mutate(purpose = reorder(purpose, average_default_rate)) %>%
  # Put the plot together
  ggplot(aes(x = purpose)) +
  # Roughly 95% confidence intervals for the proportions
  geom_point(aes(y = proportion_defaulted, colour = grade, size = N)) +
  geom_line(aes(y = proportion_defaulted, colour = grade, group = grade)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = -90)) +
  labs(title = "Default rates by loan Lending Club grade and loan purpose")
```

What do we see? First, there does indeed appear to be a difference within
Lending Club grades across different loan purposes. Second, the worse grades 
have far smaller sample sizes---which makes sense; a lender wouldn't want to put 
many eggs in a very risky basket! This makes the riskier loans seem far more 
volatile; this might be so, but it could just be the smaller sample sizes.
Although there are differences across loan purposes, the majority of the
differences appear to be across loan grades. 

We would expect that those who default on their debts might be marginally attached to the workforce, 
or at risk of losing their job. This is especially relevant for the dataset we're looking at, which 
contains consumer loans during the perior of the global financial crisis and US recession. 
Let's examine a few of these variables. 

First let's plot income (in log 10 units) against the (jittered) binary default indicator, and include a non-linear
regression curve. These plots are extremely powerful; you can at once see the distribution of outcomes,
the distribution of the variable on the x-axis, and, by overlaying a non-linear regression curve, 
the approximate conditional probability of default for a given person whose income is at the point along 
the x-axis.

```{r, echo = T}
options(scipen = 999)
mini_loans %>% 
  ggplot(aes(x = annual_inc, y = defaulted)) +
  geom_jitter(alpha = 0.01, height = 0.05) +
  geom_smooth() +
  scale_x_log10() +
  theme_bw() +
  labs(title = "Probability of default by income")
```

We can do something similar with the years' employment; for this we will need to extract numbers from the employment
length to perform ordering. 

```{r, echo = T}
mini_loans %>%
  mutate(years_employment = parse_number(gsub("< 1", "0", emp_length))) %>% 
  ggplot(aes(x = years_employment, y = defaulted)) +
  geom_jitter(alpha = 0.01, height = 0.05) +
  geom_smooth() +
  theme_bw() +
  labs(title = "Probability of default by years' employment")
```

Interestingly, we don't see any clear relationship between job tenure and default probability. 

Finally, let's look at how a borrower's debt servicability affects repayments. There are several variables
commonly used to describe debt servicability, including the revolving debt utilization ratio. Let's look at this.

```{r, echo = T}
mini_loans %>% 
  ggplot(aes(x = revol_util, y = defaulted)) +
  geom_jitter(alpha = 0.01, height = 0.05) +
  geom_smooth() +
  theme_bw() +
  labs(title = "Probability of default by utilization ratio") +
  xlim(0, 100)
```

This tells us something we might already expect: that those who are using more of their available credit lines
are far more likely to default than those who are not. 

Above we've performed a quick exploratory analysis of a real-life commercial dataset. We've learned that 
credit scores and loan purposes are both strongly associated with default risk, as is a high revolving utilization 
rate, and a low income. 

Now we're on our way to building a model, but not before building an understanding of the bits and pieces that go 
into building a good model. The next secion describes these. 

\newpage 

## Part 3: A gentle introduction to required knowledge for modeling 

This book is about modeling, and so it makes sense to describe what we think of as being a model. By thinking about models
holistically, you'll find that you can move between domain areas with relative ease. The concepts in this section
are just as applicable to economics as they are to pharmacometrics or natural language processing or computer vision. It's 
all essentially the same! Fundamentally, any model has the following: 

- A collection of "knowns"---variables that we treat as being known for sure, or assume to take
particular values. If we're building a predictive model of whether a borrower defaults on a loan, this might be the information
we have about the customer at the time we will be called on to make predictions. If we're a government choosing a fiscal policy, 
it might be a projected spending pattern (even though this is not "known", for a modeling exercise we might pretend it is known
for the sake of some policy simulation). In some cases, especially in time series and computer vision, 
the set of "knowns" might be empty. We often use $X$ for "knowns". 
- Some "model unknowns", normally called $\theta$. These might be latent variables, coefficients, scales, weights, 
etc. Throughout this book we'll use the term "model unknown" rather than the more common "parameter", which we'll reserve
for a specific use. In principal, we can build any model and plonk values in place of these model unknowns, and perform
some analysis (thought that analysis might be bunk). In fact, many applications of economics---particularly 
computable general equilibrium modeling---involve "calibrating" models in precisely this way, using 
"reasonable" assumed values for model unknowns. Poorly chosen values
of these unknowns will typically make the model say useless things, and most of this book is about strategies to come up
with good values for these unknowns. 
- A "model structure", often denoted by $\mathcal{M}$. This is a mathematical structure that provides some mapping between 
the coupling of our knowns $X$ and unknowns $\theta$, and some outcomes that we're interested in, $y$. In economics, this might be
as simple as linear model, or as complex as a large auction model or dynamic macroeconomic model. Outside economics, this
could be a neural network for natural language processing, a Gaussian process for modeling voter preferences, or a 
set of differential equations to model the impact of a drug on a biological system. 
- A collection of "outcomes", $y$. These are the variables whose values we're interested in, either because we're trying to 
generate predictions for them in the future, or because we're trying the generate predictions for them under different 
policies or treatments. In economics these might be prices and sales, or macroeconomic variables like employment and 
inflation. Outside economics the "outcome" might be whether a picture is of a "cat" or a "horse"; or perhaps
whether an ill person recovers or dies of their illness.

The sorts of models discussed in this book are *generative* or *probabilistic* models. Such models are probabilistic in the sense
that for a given set of "knowns" $X$ and fixed values of the model unknowns $\theta$, there are many 
plausible outcomes that could occur, and the outcomes we observe in our data are only one of many possible 
outcomes that might have happened. The world which we're trying to approximate with a model *generates* 
values for the outcomes, and so too should our model. More formally, a generative model is 
a _joint probability distribution_ over the outcomes and model unknowns, holding the model 
structure and known information fixed. 

```{r observedunobserved, caption = "Fixed values of model knowns and unknowns pass through our generative model and result in many possible outcomes"}
knitr::include_graphics('observed_unobserved.png', dpi = 192)
```

Again: fixed values of the model unknowns and knowns, when given to our generative model, will be able to "generate"
some outcomes. If you give _the same_ values of knowns and unknowns to the model and ask it to generate some outcomes 
another time, it will give you different outcomes. When performing generative modeling, 
we don't start with the assumption that there's a deterministic one-to-one relationship between our inputs and outcomes. 
Instead we say that there is a _probabilistic_ relationship between them. 

Our challenge when modeling is to gain the best possible understanding of the values of the unknowns for some new
data for which we want to perform prediction or policy analysis. This means "inverting" the relationship above: 
if a given set of unknowns combined with a model and some knowns can generate many plausible outcomes, then
each set of outcomes might be associated with many possible unknowns. If the outcomes cannot pin down
the unknowns, we call the model *unidentified*. For example, the linear model $y_{i} = \beta \times 0 + \epsilon_{i}$ is 
unidentified. Any value of $\beta$ could be consistent with some observed data. When the data are able to pin 
down the unknowns, we say the model is *identified*. This language can be confusing in economics, where the term "identification"
is normally reserved for the process of identifying parameters in a causal model. We'll look more at this problem in 
chapter 2. 

Now that you have a basic understanding of the various bits of a model, let's step back a bit and 
cover some required knowledge for building and fitting models. This will seem a little abstract at first, 
so we've included some fairly simple examples. 

\newpage

### Random variables and their distributions

The Bayesian approach to model building is centered around estimating the _distributions_ of _random variables_. 
This section described what we mean by these terms. At first we'll just discuss random variables and their 
distributions in a fairly abstract sense; that is, without relating them to modeling. Once we're familiar with 
random variables, we'll describe how we use these ideas to help us build models. 

**What is a random variable?**

*Random* or *stochastic* variables are simply variables whose values result from some random phenomenon, or 
some process that we don't fully understand. A more formal definition involving basic measure theory 
can be found (here)[https://betanalpha.github.io/assets/case_studies/probability_theory.html]. When random
variables can take an uncountably infinite number of values, they are said to be *continuous*; when they
take on a countable number of values, they are *discrete*. 

Often a generic random variable is denoted $X$,
which should not be confused with the "knowns" in our model taxonomy above. A generic realization of a random variable
by convention is in the lower case, so $x$ would be a realization of $X$. For example, $X$ might be "Sarah Smith's 
potential income in 2018", while $x$ might be "the amount that Sarah Smith actually earned in 2018". Note here
that the random variable here "Sarah's potential income" is entirely hypothetical---random variables are a 
convenient way of representing and _idea_, and are not necessarily observed data. We'll get to the relationship between
random variables and our data later in this section. 

**All random variables have probability distributions**

The point of using Bayesian methods is to understand the _joint probability distribution_ of a collection of random
variables. Let's set aside the _joint_ part of this and briefly define a probability distribution. 
A probability distribution, also known as a _probability density_ (for continuous random variables)
or a _probability mass_ (for discrete random variables) is a function that describes the relative probability of observing
various values of a random variable. Formally, a probability distribution is a function $p()$ that assigns 
some non-negative value to each possible realization of a random variable, and sums (integrates) to 1 over all
possible values of the random variable. That is, a probability distribution says that _some outcome_ must happen, 
and describes the relative probabilites of each. If $p()$ assigns a value of 0 to some value of a random 
variable (formally, $p(X=x) = 0$ for some $x$), we do not think it's possible to for $X$ to take on the 
value $x$. If $p(X=x_{1}) > p(X= x_{2})$, it is more probable to observe a value of $X$ with the 
value $x_{1}$ than $x_{2}$. 

The notation for probability distributions $p()$ is _overloaded_, which is to say that if we have two 
random variables $X$ and $Y$, then the function $p(X)$ is different to the function $p(Y)$. And neither
of these function need to have a neat analytical form. When we can represent $p()$ with a mathematical
expression with a fixed number of parameters, then we say that $p()$ is a *parametric* distribution 
function. If we cannot express $p(X)$ analytically, then $p(X)$ is *nonparametric*. A great deal of 
statistical modeling involves finding ways of approximating nonparametric distributions with parametric ones, 
and finding ways of extracting information from nonparametric distributions efficiently. 

If we know the distribution of a random variable with certainty, then we can also make probabilistic 
statements about the random variable falling within a set of values. Yet we typically do not know the 
distribution of the random variable, and have to infer what it might be. 



From this distribution, we can make probabilistic statements, like "The probability of a randomly chosen male
between 30-39 being less than 5'4" is 3.1%." The distribution above is known as an _empirical distribution_, 
the actual distribution of some data---in this case, from a population. When we bucket the data and present it like
this it is is also known as a _histogram_. 

Such a distribution is said to be _nonparametric_, in that we aren't using a parameterized analytical function 
to describe the relative probabilities of various heights. Note we can always draw a histogram of our data, 
indeed doing so is an important part of the modeling process. 

In this example, we can plot the "true" empirical distribution of the data. First, human heights are _observable_. 
Whenever a variable is observable, we can calculate its distribution directly, at least for 
the sample of data that we have at hand. (There is undoubtedly some measurment error, but we'll ignore that for now). 
Second, this is data from the census, so  the distribution of heights that we observe is approximately equal 
to the true distribution of heights in the population. In most applied modeling work, few of these properties 
are satisfied; we often don't observe what we are truly interested in, and don't observe data at the population 
level. And so we need to make some assumptions. 

When we build economic models, we're almost always interested in unobservables. If we are performing causal inference 
on a public policy experiment, we're interested in _treatment effects_---how much a policy impacts various stakeholders---
which we never truly observe. A macroeconomist might want to know the impact of some fiscal policy on unemployment in a 
counterfactual state---again, unobserved. And engaging in a forecasting exercise involves making inference about an unknown, 
the future values of some series. 

Just as applied economists aren't estimating the distributions of observed variables, applied economists rarely 
observe population-level data, relying on samples drawn from potentially biased sources. 
This isn't always the case; there is an exciting literature that makes use of population-level data from taxation agencies, for instance, Chetty (bunch of papers) and Kleven (some more). Yet most of the time we're being asked to draw inference for
some new population (or the current population in the future). 

When it comes to unobserved random variables, we're left with a sad reality: we cannot see them! So we can't calculate
their empirical distribution.  In order to make the analysis tractable, we need to make some more assumptions. In classical 
econometrics, these assumptions tend to center on the limiting distribution of random variables as the sample size approaches
infinity. In many cases, for instance the tax studies by Chetty and Kleven cited above, these assumptions are very 
likely to be satisfied. Yet when our data are of a more limited size, or if the number of unknowns is high relative to
the sample size, then we will need to make some further assumptions in order to say anything meaningful about
the random variables in our model. That's where Bayesian modeling comes in. 

**How do we use random variables in our modeling?**

Here are two questions for you. Do you know how much you are going to earn over the next three years? 
And are you aware how much reading through this book will affect those earnings? These sorts of questions get to the nub
of much economic modeling: we want informed (probabilistic) forecasts and estimates of variables that might
take on many possible values (and might do so because of random causes). 

Random variables are variables like those alluded to in these two questions. The first question 
asks us to make a _forecast_ about a variable (our income) that may take on many values (what if there is a recession,
or if you change jobs, etc?). This is a random variable. The second question asks us for a _causal estimate_ 
of this book on your earnings. In reality, this "treatment effect" is probably fixed, but our understanding of it 
will never be, and so we want to treat our inference of this value as a random variable. 

What does it mean to "treat something as a random variable"? In practical terms, this means that 
we are up-front in saying that we do not know its true value _out of sample_. When we say "out of sample"
we mean "for the yet-unobserved data for which we want to say something useful." This might be a broader
population, or a specific population (future customers, say), or time periods in the future, etc. The entire point
of building models is to be able to say something meaningful about the probable values of 
these random variables in a useful context. 

Two more questions: do you know what sex you will be in two years' time? How about what day 
it will be in ten days' time? These are similar "forecasting" questions, but most people would 
be able to answer them with something close to certainty. If we were to be using the day of the 
week or a person's sex in a model to help predict some (random) outcome, we would not typically need to treat
them as random variables. When we use these variables in a model, we typically call them _covariates_, 
_predictors_ or in the machine-learning world, _features_.


### Analytical (parametric) distributions

In most Bayesian applications, we make liberal use of parametric distribution functions to help 
us define and fit models. The type of distributions we use depends on the random variable we are modeling. 
Any function that satisfies the following can be used as a distribution function: 

1. Must give non-negative values to values of the random variable; 
2. Must integrate to 1 over the domain of the random variable;
3. The value of the function at $x$, $p(x)$, defines the relative probability of observing the value of $x$. 

There are a huge number of possible parametric distributions that we can use, but the ones we choose
will typically be decided by the nature (and constraints) of our data. Each parametric distribution 
is characterised by _parameters_. Typically, different values of these parameters imply different 
distributions of the random variables. 

Below we list the sorts of things we need to check (typically by data visualization) in our data to 
help us choose the right distributions, as well as describing some commonly-used distributions. 

#### Probability mass functions

Probability mass functions assign _probabilities_ to various values of a discrete random variable.
There are many types of discrete random variables and things we should look out for: 

- **Binary/dichotemous or logical data**
    - In the canonical case, this is the toss of a weighted coin in which each face comes up with some 
  probability. In industrial applications this might be whether or not a customer makes a purchase/choice, 
  whether or not the economy is classified by the NBER as being in default, etc. This sort of data has
  a _Bernoulli distribution_. 

- **Ordinal data**
    - A type of discrete data when the actual value is not observed, but the rank ordering is. When a survey asks
    us to rank a set of alternatives, the result is ordinal data. It does not measure _how much_ we value one 
    alternative relative to another, but does constrain the possible cardinal values. We typically use a 
    generalization of the Bernoulli distribution to model ordinal data. 

- **Categorical data**
    - When our outcome variable is an unordered discrete variable (for instance, which product you choose
    choose from a set of alternatives, or a person's ID code, then the data is "categorical". When modeling
    this sort of data we use the _categorical distribution_, which can be thought of as rolling a weighted dice. 
    If our data is counts of categorically distributed data then we use the _multinomial distribution_. The 
    categorical distribution is a special case of the multinomial distribution (one with a single count). 
    

- **Count data**
    - Often our data are counts of an event, for example the a discrete number of purchases. This sort of data 
    can be unbounded or unbounded. An example of bounded counts would be "given 100 customers were in the
    story today, how many made a purchase?" Such count data is normally modeled using the Binomial distribution. Unbounded
    count data, especially that referring to the number of events that occurs over a fixed interval of time, 
    is often modeled using the Poisson distribution. For example, "how many goals will Everton kick in a game?"
    - Count data as in these examples often have a couple of characteristics that these distributions not fit the 
    data particularly well. The first is _zero inflation_---we observe more 0 counts than the distribution would
    suggest is probable. The second is _over-dispersion_, where we observe a wider range of counts than would be
    expected by the Poisson distribution. It's fairly straightforward to include these characteristics in a 
    parametric mass function. 

#### Probability density functions

When our random variable is continuous, we use _probability density functions_ to describe its probabilistic distribution. 
These functions describe the relative likelihood of observing various values of a random variable. They also describe the 
probability of observing values of a random variable within a range. For example, take a standard normal density (that is, 
a normal density with mean $\mu = 0$ and scale $\sigma = 1$. To get the probability that a random variable with this distribution
falls in the interval $(-0.5, 1)$ all we need to do is integrate the density between those values

$$
\text{Given } x \sim \text{Normal}(0, 1)
$$
$$
p(-0.5 < x <1) = \int_{-0.5}^{1}\text{Normal}(x |\, 0, 1) dx
$$

This is illustrated below

```{r}

bounded_dnorm <- function(x, mean, scale, lower, upper) {
  ifelse(x<= upper & x >= lower, dnorm(x, mean, scale), 0)
}


ggplot(data.frame(x = c(-5,5))) +
  stat_function(aes(x = x), fun = dnorm) +
  labs(title = "Normal(0, 1) density evaluated\nat -0.5 and 1",
       y= "Density") +
  theme_hc(base_size = 8) +
  geom_point(aes(x = c(1, 1), y = c(0, dnorm(1, 0, 1))) ) +
  geom_linerange(aes(ymin = 0, ymax = dnorm(1, 0, 1), x = 1))+
  geom_point(aes(x = c(-0.5, -0.5), y = c(0, dnorm(-0.5, 0, 1))) ) +
  geom_linerange(aes(ymin = 0, ymax = dnorm(-0.5, 0, 1), x = -0.5)) +
  stat_function(fun = bounded_dnorm, args = list(mean = 0, scale = 1, lower = -0.5, upper = 1),geom = "area", alpha = 0.3, n = 1000) +
  annotate("text", x = 0.25, y = 0.15, label = paste0("Prob\n = \n", round(pnorm(1) - pnorm(-0.5), 2))) +
  annotate("text", x = -0.5, y = -0.02, label = "-0.5") +
  annotate("text", x = 1, y = -0.02, label = "1") 
  
```

Let's quickly discuss the notation used above. Above, we said "$\text{Given } x \sim \text{Normal}(0, 1)$". This can be read as "given
that x (a random variable) is distributed with a normal density with location parameter 0 and scale parameter 1". Often we 
use $\sim$ to denote a "sampling". For instance, you could generate draws of $x$ by drawing from a standard normal distribution. 
If we wanted 1000 draws, we could do this in R like so: 

```{r, echo = T}
x <- rnorm(1000, mean = 0, sd = 1)
```

or just 

```{r, echo = T}
x <- rnorm(1000) # by default it's a standard normal
```

Just as with discrete random variables, we typically choose the type of continuous density
based on the nature of the random variable we're modeling. The "default" is almost always
a normal distribution, yet it often should not be. A few things we have to look out for: 

- **Is the distribution bounded?**
    - Often random variable has a natural bounding; for example, many prices should be positive, 
    and for normal goods, we expect price elasticies of demand to be negative. Others are unbounded: 
    GDP growth or inflation could be positive or negative. 
    - When a random variable is unbounded, we use unbounded distributions. Commonly, the Normal
    distribution, the Cauchy, Student's T, Double Exponential, and Gumbel distributions are used. 
    - When the random variable is bounded, we use either truncated versions of the unbounded distributions above 
    (these give no probability to values outside the bounding), or distributions that have a natural
    bounding. Examples of the latter include the Gamma/inverse Gamma distribution, the exponential, 
    and the lognormal distribution. Four examples are illustrated below. 
    
```{r}
library(truncnorm)

truncated_student_t <- function(z, nu = 4) {
  ifelse(z< 0, 0, dt(z, df = nu)*2)
}

truncated_dists <- ggplot(data.frame(x = c(-1,5))) +
  stat_function(aes(x = x), fun = dtruncnorm, args = list(a = 0, b = Inf), n = 1000, colour = "red") +
  stat_function(aes(x = x), fun = truncated_student_t, n = 1000) +
  labs(title = "Truncated Normal and Student's T densities") +
  annotate("text", x = 1.5, y = 0.7, label = "Half (standard)\nNormal", colour = "red") +
  annotate("text", x = 3.5, y = 0.2, label = "Half Student's t\ndf = 4") +
  theme_hc()
  
positive_dists <- ggplot(data.frame(x = c(-1,5))) +
  stat_function(aes(x = x), fun = dgamma, args = list(rate = 4, shape = 2), n = 1000, colour = "red") +
  stat_function(aes(x = x), fun = dexp,args = list(rate = 2),  n = 1000) +
  labs(title = "Gamma and Exponential densities",
       subtitle = "Are naturally bounded to positive values") +
  annotate("text", x = 1.5, y = 0.7, label = "Gamma density\nrate = 4\nshape = 2", colour = "red") +
  annotate("text", x = 3.5, y = 0.3, label = "Exponential density\nrate = 2") +
  theme_hc()

gridExtra::grid.arrange(truncated_dists, positive_dists)

```
    

- **Does the random variable have "fat tails"?**
    - Often we want to allow for "fat tails"---values of the random variable that are a long way from the bulk
    of the distribution. Common use-cases are in modeling financial markets, where big positive and negative 
    jumps in asset prices happen with some frequency. We also often use fat-tailed prior distributions (discussed
    later in this chapter) to allow for the possibility that the true value of the unknown lies a long 
    way from our prior. 
    - Many densities have fat tails; for unbounded densities we often use Student's T, Cauchy and Gumbel densities; for positively-constrained variables we often use Exponential and Lognormal densities. 

    
**Joint distributions**

We are often interested in the _joint_ behaviour of random variables. 

- Joint distributions
  - Correlation
  - Covariance

#### Why parametric distributions? 

In Bayesian modeling, we use parametric distributions for three main purposes. First, to describe the
distribution of the observable variables that we are modeling (given the unknowns in the model, and the 
"knowns" that we're not modeling). For example, if we're modeling the joint distribution of unemployment, 
wages growth and GDP growth, then we would choose a parametric distribution for their joint behaviour
given the model unknowns and things that we don't want to model (say, fiscal policy). A wide variety of 
parametric distributions, and even "mixtures" of them, can approximate many data that we see in the wild. 

The second use is to allow us to summarise our prior information about model unknowns before fitting the 
model. We discuss priors later in this chapter, but all you need to know for now is that we almost always
use parametric distributions as priors. 

Finally, we use parametric distributions to help us evaluate whether the model unknowns we propose
are good or bad. In this sense, parametric distributions can act like _loss functions_ (to borrow a 
term from machine learning), returning larger values when "better" unknowns are proposed. We discuss
this more thoroughly in the Likelihood section of the chapter. 

#### Parametric distributions have parameters

The defining characteristic of parametric distributions is that their shape is characterized by 
a set of parameters. For instance, the normal distribution has a location (mean), often $\mu$, 
and scale (standard deviation), often $\sigma$. Sometimes you will see the normal distribution
"parameterized" in a different way, for instance with a location $\mu$ and variance $\sigma^{2}$. 
Or perhaps using the location $\mu$ and "precision" $\tau = 1/\sigma^{2}$. Often it can be convenient
to reparameterize models like this to aid in interpretability of estimates, or for computational reasons. 

When we write

$$
y_{i} \sim \mbox{Normal}(\mu, \sigma)
$$
we read this as "each observation $i$ of the random variable $y$ is distributed normally with location $\mu$ and scale $\sigma$". Given this information, we can calculate any statistic we want about the distribution 
of yet-unseen values of $y$. For instance, the expected value $E[y] = \mu$, the standard deviation is $\sigma$, 
and so the variance $\sigma^{2}$ and so on. 

The entire game of model building using parametric distributions is to replace the parameters with functions 
which may contain unknowns. For example, the normal linear model 

$$
y_{i} = X_{i}\beta + \epsilon_{i} \text{ with } \epsilon_{i}\sim \text{Normal}(0, \sigma)
$$
can be written 

$$
y_{i} \sim \text{Normal}(X_{i}\beta, \sigma)
$$
Note what we've done---we've simply replaced the location $\mu$ with a function describing the 
conditional mean of observation $i$, $X_{i}\beta$. In this model, each observation can be centered
around a different point ($\mu_{i} = X_{i}\beta$), but all have the same error scale around
this point $\sigma$. This is a simple linear function, but there's nothing stopping us from replacing it 
with something that contains more information---for example, it might be a function that finds 
a Nash Equilibrium of some game (we'll see these in chapter 4). Just as easily, we could let each 
observation have their own error scale, $\sigma_i = \exp(X_{i}\gamma)$

$$
y_{i} \sim \text{Normal}(X_{i}\beta, \exp(X_{i}\gamma))
$$
Most commonly, scale parameters are modeled explicitly in the finance and macroeconomics literature---we 
cover several such models in chapter 5. A number of common distributions and their parameter names
are given on the following page. 

\newpage

```{r manydistributions, fig.cap= 'A collection of commonly-used distributions and their parameter names', echo = F, fig.height = 12}
library(dplyr); library(ggplot2); library(ggthemes); library(gridExtra)

a  <- ggplot(data.frame(x = -5:5)) +
  stat_function(aes(x = x), fun = dnorm) +
  labs(title = "Normal(), location = mu, scale = sigma",
       y= "Density") +
  theme_hc(base_size = 8)

b <- ggplot(data.frame(x = 0:1)) +
  stat_function(aes(x = x), fun = dbinom, args = list(size = 1, prob = .3), geom = "bar", width = .6, fill = "grey", alpha = 0.7) +
  labs(title = "Bernoilli(), probability = theta",
       y = "Probability") +
  theme_hc(base_size = 8)+
  scale_x_continuous(breaks = 0:1, labels = 0:1)

c <- ggplot(data.frame(x = 0:5)) +
  stat_function(aes(x = x), fun = dlnorm) +
  labs(title = "Lognormal(), location = mu, scale = sigma",
       y= "Density") +
  theme_hc(base_size = 8) 


d <- ggplot(data.frame(x = 0:1)) +
  stat_function(aes(x = x), fun = dbeta, args = list(shape1 = 3, shape2 = 2)) +
  labs(title = "Beta(), alpha, beta",
       y= "Density") +
  theme_hc(base_size = 8) 

e <- ggplot(data.frame(x = -5:5)) +
  stat_function(aes(x = x), fun = dcauchy) +
  labs(title = "Cauchy(), location = mu, scale = sigma",
       y= "Density") +
  theme_hc(base_size = 8) 

f <- ggplot(data.frame(x = 0:5)) +
  stat_function(aes(x = x), fun = dgamma, args = list(shape = 3, scale =.3)) +
  labs(title = "Gamma(), shape = k, scale = alpha",
       y= "Density") +
  theme_hc(base_size = 8) 


g <- ggplot(data = data.frame(x= 0:40)) +
  stat_function(aes(x = x), fun = dpois, args = list(lambda = 13), geom = "bar", width = .6, fill = "grey", alpha = 0.7) +
  labs(title = "Poisson(), shape, scale = lambda",
       y= "Probability") +
  theme_hc(base_size = 8)

h <- ggplot(data.frame(x = c("a", "b", "c"), Probability = c(.3, .5, .2)), aes(x =x, y = Probability)) +
  geom_bar(stat = "identity", fill ="grey", alpha = 0.7)+
  labs(title = "Categorical(), probability = theta") +
  theme_hc(base_size = 8)

grid.arrange(a, b, c, d, e, f, g, h, nrow = 4, top ="Some common parametric distributions")
```


\newpage

The following section describes how we can use parametric distributions---chosen with respect to our data---
to evaluate the likelihood function of the data given a proposal of some unknowns. This is a step towards fitting
models using Bayesian techniques. 

### Histograms


## Part 4: The likelihood function

In order to fit Bayesian models we need to construct a function that tells us when 
certain values of model unknowns are good or bad. For example, in image \@ref(goodbaddensity) below,
we plot a histogram of values of some random variable X. We want to fit a density function to this histogram
so as to be able to make probabilistic statements about the likely distribution of yet-unseen observations. 
To gauge which proposed density is a good one, we would like a function that gives a higher value for 
the proposed model unknowns that lead to the distribution in the bottom panel and lower values for 
the proposed model unknowns that lead to distribution in the top panel. 

```{r goodbandensity, fig.cap= 'A likelihood function would return a higher value for the proposed density on the bottom than the proposed density on top.'}

df1 <-  data_frame(X = rnorm(100, 3, 3))
dens_1 <- df1 %>%
  ggplot(aes(x = X)) +
  geom_histogram(aes(y = ..density..), alpha = 0.2) +
  stat_function(fun = dnorm, args = list(mean = -1, sd = 1)) +
  theme_hc() +
  labs(title = "A bad proposed density",
       subtitle = "Proposed location = -1, proposed scale = 1") +
  ylim(0, .4)

dens_2 <- df1 %>%
  ggplot(aes(x = X)) +
  geom_histogram(aes(y = ..density..), alpha = 0.2) +
  stat_function(fun = dnorm, args = list(mean = 3, sd = 3)) +
  theme_hc() +
  labs(title = "A good proposed density",
       subtitle = "Proposed location = 3, proposed scale = 3")+
  ylim(0, .4)

gridExtra::grid.arrange(dens_1, dens_2)

```

There are many functions that we could use to determine whether some proposed model unknowns 
result in a "better" fit than some other unknowns. Likelihood functions are one approach, based
on the assumption that the proposed _generative distribution_ gave rise to the observed data. 
This is most easily motivated with an example. 

**Example: the binomial likelihood of loan defaults**

Let's say we wanted to estimate the probability of loan defaults. We have a small sample---only five loans---
and one has defaulted. If we encode defaulted = 1 and not defaulted = 0, then our data look like

```{r}
c(0, 0, 1, 0, 0)
```

The likelihood function answers the question "what is the probability of observing this sequence if the 
probability of defaulting is $\theta$?" In the case that the outcomes are independent of each other, then
this is the product of the _likelihood contributions_---the probability of observing each observation. The 
probability of observing a non-default is $(1 - \theta)$, and so the probability of observing a default is 
$\theta$. For example, let's say we think the probability of defaulting is 2%. Then our likelihood (the product of 
the likelihood contributions) would be 

$$
p(y = (0, 0, 1, 0, 0)'|\, \theta = 0.02) = 0.98 \times 0.98 \times 0.02 \times 0.98 \times 0.98 = 0.0184
$$

The above is the specific case to this example. In general, the Bernoilli likelihood---the likelihood function
we use for binary outcomes---is 

$$
p(y |\, \theta) = \prod_{i=1}^{N} \theta^{y_{i}}\times(1 - \theta)^{1 - y_i}
$$

Note that when $y_{i} = 0$---no default---$\theta^{y_{i}} = 1$. Now, is the proposed probability 
of defaulting a good one? That's what we can use the likelihood function to perhaps determine. We 
plot the values of the likelihood function for this data evaluated over the possible values of 
$\theta$ below. 

```{r bernoillilikelihood}
bern_l <- function(theta) {
  y <- c(0,0,1,0,0)
  prod(theta^(y)* (1 - theta)^(1 - y))
}
ggplot(data.frame(theta = 0:1), aes(x = theta)) +
  stat_function(fun = Vectorize(bern_l), n = 1000) +
  theme_hc() +
  labs(title = "Likelihood function for y given theta", 
       y = "Likelihood") +
  geom_linerange(aes(x = 0.2, ymin = 0, ymax = bern_l(0.2)), colour = "red", alpha = 0.3) +
  annotate("text", x = 0.2, y = 0.085, label = "Likelihood at 0.2", alpha = 0.5, colour = "red")
```

What do we notice about this likelihood? First, it's _not_ a probability distribution
in $\theta$---it does not integrate to 1. This is a common source of confusion, firstly because it looks
like many of the distributions we're used to working with. But also because we write it as $\text{Likelihood} = p(y|\, \theta)$. 
It _does_ integrate to 1 over the possible values of $y$, but not over the possible values of $\theta$. Second, 
we notice that in this case it is maximized at 0.2, which happens to be the proportion of loans in our very 
small sample that have defaulted. The maximum point of the likelihood function is known as the _maximum likelihood estimate_ (or MLE) 
of our parameter $\theta$ given our data. Formally, 

$$
\hat\theta_{MLE} = \text{argmax}_{\theta}(p(y |\, \theta))
$$
The plot illustrates something potentially dangerous about using the Maximum likelihood estimate with small
samples. In this case, before we did the analysis, we had reason to believe that the default rate was 2%. This is what we might call 
_prior information_; information that exists from sources other than the sample of data we're analyzing. 
Unfortunately our maximum likelihood estimate does not reflect this prior knowledge at all, and we're left
with a probability of default simply equal to the sample frequency. Later in this chapter we'll discuss
how to encode this outside information in our estimate. 

**The Bernoulli log likelihood**

In this simple example, in which we have only one unknown and very few data points, we have no problem 
evaluating the likelihood function directly. Yet you might have noticed that as we provide more observations
of $y$ to the function, it will get closer to 0---in the discrete case, the likelihood contribution of each
observation is between 0 and 1, so multiplying more and more of them together results in a rapidly shrinking 
likelihood value. If you had a large number of observations for example, your computer will simply not be able
to handle that number of leading 0s. 

```{r, echo = T}
# draw 100k 0-1 coin flips
y <- sample(0:1, 1e5, replace = T) 

# A bernoulli likelihood function
bernoulli_l <- function(theta, y) {
  prod(theta^y * (1 - theta)^(1 - y))
}
# evaluate likelihood at the correct value
bernoulli_l(0.5, y)
```

In order to handle these issues with numerical precision, we use the _log likelihood function_. This is, 
as the name suggests, the log of the likelihood function. In the Bernoulli case, it's 

$$
\log(p(y |\, \theta)) = \log\left(\prod_{n=1}^{N} \theta^{y_{i}}\times(1 - \theta)^{1 - y_i}\right) = \sum_{i = 1}^{N} (y_{i}\times \log(\theta) + (1 - y_{i})\times \log(1 - \theta))
$$

Which makes use of the fact that $\log(ab) = log(a) + log(b)$ and $\log(a^{x}) = x\log(a)$. This representation of the likelihood is
far easier for us to work with than the raw likelihood. For one, it is order preserving---the values of the unknowns that maximize
the log likelihood are the same as those that maximize the likelihood---and yet we sum the log likelihood contributions, so small 
probabilities don't send the value towards 0. Second, in some cases (for instance, with a binomial or categorical
likelihood), you can evaluate it with linear algebra. If we want the likelihood of a vector $y = (0, 0, 1, 0, 0, \dots)'$ evaluated with the
probability vector $\theta = (\theta_{1},\, \theta_{2},\dots)'$ denoting the probability of each observation, then the log likelihood is just 

$$
\log p(y |\,\theta) = y'\log(\theta) + (1 - y)'\log(1 - \theta)
$$

We use this formulation extensively in Chapter 4. 

Now that we have a basic understanding of the log likelihood function, let's take a look at the log likelihood function
for a model with a continuous outcome and three unknowns. 

**Log likelihood of a simple normal linear model**

Let's say we have a very simple linear model of $y$ with a single covariate $x$. $\alpha$, $\beta$ and $\sigma$ are the model's parameters. 

$$
y_{i} = \alpha + \beta x_{i} + \epsilon_{i}
$$

with $\epsilon_{i} \sim \mbox{N}(0, \sigma)$. This is the same as saying that 

$$
y_{i} \sim \mbox{N}(\alpha + \beta x_{i}, \sigma)
$$

This is the model we are saying _generates_ the data---$\mbox{N}(\alpha + \beta x_{i}, \sigma)$ is 
the _generative distribution_. It has three unknowns, $\alpha$, $\beta$ and 
$\sigma$. If we knew what these values were for sure, we could make probabilistic statements about 
what values $y_{i}$ might take if we knew the value of $x_{i}$. 

But that's not how data analysis normally works. Normally we're given values of $x$ and $y$ (or many $x$s and
many $y$s) and have to infer the relationship between them, often with the help of some model. Our task
is to estimate the values of the unknowns---in this case, $\alpha$, $\beta$ and $\sigma$. 

So how do we do this by maximum likelihood? One thing that computers are relatively good at is choosing values
of some unknowns in order to optimize (normally minimize) the value of some function. Here we use the normal log
likelihood function as a way of scoring various combinations of $\alpha$, $\beta$ and $\sigma$ so that
the score is high when the model describes the data well, and low when it does not.

Let's say we propose a set of parameters, $\alpha = 2$, $\beta = .5$ and $\sigma = 1$ and we have an 
obervation $y_{i} = 1,\, x_{i} = 1$. Given the parameters
and $x_{i}$ we know that the outcomes $y$ should be distributed as

$$
y_{i} \sim \mbox{N}(2 + .5 \times 1, 1) = \mbox{N}(2.5, 1)
$$

which might look like this: 

```{r, message = F, warning = F, echo = F}
ggplot(data.frame(x = c(-1, 5.5)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = 2.5, sd = 1)) +
  xlab("y") +
  ylab("Density") +
  ggtitle("A predictive density given alpha, beta,\nsigma, and x") +
  theme_bw()
```

Now we ask: what was the density at the _actual_ outcome $y_{i}$?

```{r, message = F, warning = F, echo = F}
ggplot(data.frame(x = c(-1, 5.5)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = 2.5, sd = 1)) +
  geom_linerange(data = data.frame(ymin = 0, ymax = dnorm(1, 2.5, 1), x = 1), aes(x= x, ymin = ymin, ymax = ymax)) +
  geom_point(aes(x = c(1, 1), y = c(0, dnorm(1, 2.5, 1))) ) +
  xlab("y") +
  ylab("Density") +
  ggtitle("We can evaluate at the actual outcome", subtitle = "f(y | x, a, b, s)") +
  theme_bw()
```

In `R`, we could go ahead and write a function that returns the density for 
a given observation. Because we'll be optimizing this, we want to give it the unknowns
as a single vector. 

```{r, echo = T}
density_1 <- function(theta, # the unknown parameters
                      x, y) {
  # This function returns the density of a normal with mean theta[1] + theta[2]*x
  # and standard deviation exp(theta[3])
  # evaluated at y.
  dnorm(y, theta[1] + theta[2]*x, exp(theta[3]))
}
```

Now let's evaluate the density at $x=1,\, y=1$ for the given likelihood values. 
Note that we've included the scale (which must be positive) as $\sigma = \exp(\theta_{3})$,
as it must be positive. If we know $\theta_{3}$, we can get back to $\sigma$ by taking its log. 

```{r, echo = T}
density_1(theta = c(2, .5, log(1)), 
          x = 1, y = 1)
```

Just as in the binary outcome case, the value of the density at an observed outcome is 
the _likelihood contribution_ $f(y_{i} | x_{i}, \alpha, \beta, \sigma)$ of a single datapoint. 
Unlike the binary case, this has no direct probabilistic interpretation except in relative terms. If the 
continous density at a given value is twice the density at another, then the relative probability of observing
the first value is twice the second. Also, the density needn't be less than one; when the likelihood is very narrow, 
we routinely see values of the likelihood greater than 1. You'll be able to see that this likelihood contribution
is maximized (at infinity) when it imposes a spike on the single observation we observe, ie, $\alpha + \beta \times 1 = 1$ and $\sigma = 0$).

```{r, echo = T}
# log(0) -> -Inf
density_1(c(0, 1, -Inf) , 1, 1)
```

**But we have more than one observation**

We have one observation, three unknowns---of course this is an unidentified model. In reality 
we have many observations. So how do we use this principle of likelihood to 
get estimates of the many model unknowns? 

If we assume (conditional) independence of the draws--that is, the value of one observation's $\epsilon_{i}$
has no influence on another's $\epsilon_{j}$, the sample likelihood of your data vector $y$ is 
the _product_ of all the individual likelihoods $\prod_{i = 1}^{N}f(y_{i} | x_{i}, \alpha, \beta, \sigma)$. 
This is identical to the binary outcomes likelihood abve. 

You can probably see the problem here---for reasonably large spreads of the data, the value of the 
density for a datapoint is typically less than 1, and the product of many such datapoints
will be an extremely small number. And so just as in the binary case, we take the log likelihood
of the data. 


$$
\log\left( \prod_{i = 1}^{N}f(y_{i} | x_{i}, a, b, s)\right) = \sum_{i = 1}^{N}\log(f(y_{i} | x_{i}, a, b, s))
$$

### So what does the likelhood mean? 

Log likelihood is a confusing concept to beginners as the values of the numbers aren't easily 
interpretable. You might run the analysis and get a number of -3477 and ask "is that good? Bad? What 
does it mean?" These numbers are more meaningful when conducting model comparison, that is, we 
get the out of sample log likelihood for two different models. Then we can compare them. The usefulness
of this approach is best illustrated with an example. 

Imagine that we have an outlier---rather than $y_{j} = 1$ as in the image above, it is -10. Under the proposed
model, we're essentially saying that the such an outcome is all but impossible. The probability of observing
an observation that low or lower is `r pnorm(-10, 2.5, 1)`. The density of such a point would be 
`r dnorm(-10, 2.5, 1)`---a very small number. But the log of its density is large in absolute value: 
`r log(dnorm(-10, 2.5, 1))`. Compare that with the log likelihood of the 
original $y_{j} = 1$ : `r log(dnorm(1, 2.5, 1))`. An outlier penalizes the log likelhood far 
more than a point at the mode, and consequently our maximum likelihood estimate will try to make sense
of the outlier by dragging the entire distribution that way. 

This idea helps us do good modeling: we want to give positive weight to outcomes that happen, and no 
weight to impossible outcomes. If in the data visualization step we notice some of these outliers (and they are
real data rather than miscoded entries---which are common) then we will want to use distributions that make these
possible. Use of these fat-tailed distributions is covered in Chapter 3. 

**Maximum likelihood**

Maximum likelihood estimators are simple: if the (log) likelihood is a unidimensional score of how well 
the data fit the model for a (potentially large) number of parameters, then we can simply run an optimizer 
that attempts to maximize this score by varying the values of the parameters. If the optimizer is able to find
us the mode of the likelihood, it gives us the maximum likelihood estimator (MLE). This mode needn't be a good
estimate of what we want to know. If we have good prior information, the MLE won't capture any of that. 
And in some cases (as with the hierarchical models illustrated in the second worked example in this chapter)
the MLE returns an objectively terrible estimate. 

Let's get some practice at writing this out in R. Going through this exercise will help you understand
what's going on under the hood in Stan. 

Suppose we have a a thousand observations. $x$ is drawn from a standard normal, $\alpha = 1$, $\beta = 2$ and
$\sigma = 3$. $y = \alpha + \beta x + \epsilon$, where $\epsilon$ has a zero-centered normal distribution with
standard deviation (scale) $= \sigma$. 

```{r, echo = T}
alpha <- 1; beta <- 2; sigma <- 3
x <- rnorm(1000)
y <- rnorm(1000, alpha + beta * x, sigma)
```

Now that we have some fake data, let's write out the log likelihood function, as before. Note that because
the optimization function we use _minimizes_ the function, we need to return the _negative_ of the log likelihood
(minimizing the negative is the same as maximizing the likelihood). 

```{r, echo = T}
negative_log_likelihood <- function(theta, # the unknown parameters
                      x, y) {
  # This function returns the log likelihood of a normal with mean theta[1] + theta[2]*x
  # and standard deviation exp(theta[3])
  # evaluated at a vector y.
  -sum(log(dnorm(y, theta[1] + theta[2]*x, exp(theta[3]))))
}
```

Now we optimize

```{r, echo = T}
estimates <- optim(par = c(1,1,1), negative_log_likelihood, x = x, y =y)

estimated_alpha <- estimates$par[1]
estimated_beta <- estimates$par[2]
estimated_sigma <- exp(estimates$par[3])

paste("estimate of alpha is", round(estimated_alpha, 2), "true value is", alpha)
paste("estimate of beta is", round(estimated_beta, 2), "true value is", beta)
paste("estimate of sigma is", round(estimated_sigma, 2), "true value is", sigma)
```


Hey presto! We just estimated a model by maximizing the likelihood by varying the parameters of our 
model _keeping the data fixed_. You should note that this method can be applied more generally in 
much larger, more interesting models.

## Part 5: Bayes' rule and priors

Bayes rule gives us a method for combining the information from our data (the likelihood) and our priors, discussed below. It says
that the (joint) probability density of our parameter vector $\theta$ is  

$$
\mbox{p}(\theta|\, y) = \frac{\mbox{p}(y|\, \theta)\, \mbox{p}(\theta)}{\mbox{p}(y)}
$$

where $\mbox{p}(y|\, \theta)$ is the likelihood, and $\mbox{p}(\theta)$ is the *prior*. We call $\mbox{p}(\theta|\, y)$ the 
*posterior*. Because $\mbox{p}(y)$ doesn't depend on the vector of unknowns, $\theta$, we often express the posterior
up to a constant of proportionality

$$
\mbox{p}(\theta|\, y) \propto \mbox{p}(y|\, \theta)\, \mbox{p}(\theta)
$$

What can you take away from this? A few things: 

- If the prior or likelihood is equal to zero for a given value of $\theta$, so too will be the posterior (which is proportional to their product---if either is zero, so too must be the posterior)
- If the prior is very peaked, the posterior well be drawn towards the prior
- If the likelihood is very peaked (as tends to happen when you have many observations per unknown), 
the posterior will be drawn towards the likelihood estimate. 

Bayes' rule also tells us that in order to obtain a posterior, all we need is a prior and a likelihood 
(and some strategy for combining them). 

### Prior distributions

Prior distributions summarize our information about the values of unknowns in our model _before_ seeing the data. 
For the uninitiated, this is the scary bit of performing Bayesian analysis, often because of fears of introducing
biases, having a poor understanding of how exactly the parameter influences the model, or knowing 
how exactly to summarize this prior information in a way that makes sense for computation (ie. which 
prior distribution to choose). Such fears are often revealed by the choice of extremely diffuse priors, 
for instance regression coefficients like $\beta \sim \mbox{Normal}(0, 1000)$. 

Don't be afraid of using priors---good prior information radically improve the performance of models, as well
as computation. You almost always have high quality information about the values parameters might take on. 
For example: 

- Estimates from previous studies;
- Some unknowns have sign restrictions (fixed costs or discount rates probably aren't negative; 
price elasticities of demand probably aren't positive);
- The knowledge that "regularization" can help prevent over-fitting;
- The scale of effects are probably known. Going to college probably won't increase your income 100000%. 

Prior distributions should be such that they put positive probabilistic weight on possible values of the model unknowns, 
and no weight on impossible values. In the simple linear model, the standard deviation of the residuals, $\sigma$ must be positive. 
And so its prior should not have probabilistic weight below 0. That might guide the choice to a distribution like a 
half-Normal, half-Cauchy, half-Student's t, inverse Gamma, lognormal etc.

In a properly-posed model to be estimated with Bayesian techniques, all model unknowns must have priors. We almost always
choose these priors from a set of parametric distributions, which themselves might have models for their parameters 
(so-called "hierarchical priors"). We look at hierarchical priors later in this chapter, and again in chapters 2 and 
3. The choice of prior depends on the type of parameter being modeled. Some common parameter types and choices include: 

|Parameter type | Common prior distribution | 
|---------------| -------------------|
|Regression coefficient | (Unbounded or bounded) Normal, Student's T, Cauchy distributions |
|High-dimensional coefficients | Double-exponential, Horseshoe prior, Finnish Horseshoe prior | 
|Scale parameter (must be positive)| "Half" Normal, Student's T, Cauchy distributions; Gamma, Exponential etc. |
|Correlation matrix | "LKJ" distribution |
|Covariance matrix | Inverse Wishard matrix | 
|Simplex (shares) | Dirichlet distribution |
|Proportion (0-1 bounded continuous) | Beta distribution |

Priors have a "regularizing" effect, meaning that they pull our estimates away from the likelihood and towards
the prior. The "tighter" the prior, the greater this regularizing effect will be. If the prior puts 
very little weight on the range of plausible values suggested by the likelihood function, 
two things will happen: first, we will very often have computational issues in the model-fitting stage. 
Second, even if we can get our model to fit, it will drag our estimates away from the likelihood and towards the 
prior. At the limit, if the data contains no information about the value of a coefficient (for example a linear 
regression $y = \alpha + \beta x + \epsilon$ where every observation of $x$ is 0), then our estimate will simply 
be our prior.

This regularizing effect of priors is perhaps the most controversial aspect of Bayesian modeling. A caricatured 
criticism (correct to some degree) is that a Bayesian model could say just about anything depending on our choice
of priors. This criticism is technically true, but does not reflect the practice of Bayesian modeling done well, 
nor the advantages of using well-chosen priors. 


## Part 6: Estimating Bayesian models

### The MAP estimate

Just as we can estimate a model using maximum likelihood, we can also estimate the _mode_ of the posterior
using _penalized likelihood_ estimation. This is a really great way to start thinking about Bayesian estimation. 

To estimate a model using penalized likelihood, we simply need to recognize that if 

$$
\mbox{p}(\theta|\, y) \propto \mbox{p}(y|\, \theta)\, \mbox{p}(\theta)
$$

then

$$
\log(\mbox{p}(\theta|\, y)) \propto log(\mbox{p}(y|\, \theta)) + \, \log(\mbox{p}(\theta))
$$
That is, our log posterior density is proportional to the log likelihood plus the log prior density
evaluated at a given value of $\theta$. So we can choose $\theta$ to maximize our posterior by 
choosing a $\theta$ that maximizes the right hand side. 

We do this in the function below. I've been verbose in my implementation to make it very clear
what's going on. The `target` is proportional to the accumulated log posterior density. To maximize
the log posterior density, we minimize the negative target. We'll choose priors

$$
\alpha,\, \beta \sim \mbox{N}(0,1) \mbox{ and } \sigma \sim \text{N}_{+}(0,1)
$$

```{r, echo = T}

negative_penalized_log_likelihood <- function(theta, # the unknown parameters
                      x, y) {
  # This function returns a value proportional to the 
  # log posterior density of a normal with mean theta[1] + theta[2]*x
  # and standard deviation exp(theta[3])
  # evaluated at a vector y.
  
  # Initialize the target
  target <- 0
  
  # Add the density of the priors to the accumulator
  # Add the log density of the scale prior at the value (truncated normal)
  target <- target + log(truncnorm::dtruncnorm(exp(theta[3]), a = 0))
  # Add the log density of the intercept prior at the parameter value
  target <- target + log(dnorm(theta[1]))
  # Add the log density of the slope prior at the parameter value
  target <- target + log(dnorm(theta[2]))
  # Add the Log likelihood
  target <- target + sum(log(dnorm(y, theta[1] + theta[2]*x, exp(theta[3]))))
  
  # Return the negative target
  -target
}
```

Now we optimize

```{r, echo = T}
estimates <- optim(par = c(1,1,1), negative_penalized_log_likelihood, x = x, y =y)

estimated_alpha <- estimates$par[1]
estimated_beta <- estimates$par[2]
estimated_sigma <- exp(estimates$par[3])

paste("estimate of alpha is", round(estimated_alpha, 2), "true value is", alpha)
paste("estimate of beta is", round(estimated_beta, 2), "true value is", beta)
paste("estimate of sigma is", round(estimated_sigma, 2), "true value is", sigma)
```


And there you go! You've estimated your very first (very simple) model using penalized likelihood. 

Before you go on and get too enthusiastic about estimating everything with penalized likelihood, be warned: 
the modal estimator will do fine in fairly low dimensions, and with many fairly simple models. But as 
your models become more complex you'll find that it does not do a good job. This is partly because the 
mode of a high-dimensional distribution can be a long way from its expected value (which is what you really 
care about). But also because you often want to generate predictions that take into account the uncertainty you have 
in your model's unknowns when making predictions. For this, you'll need to _draw samples_ from your posterior. 

We'll not get there just yet---that's for another post. But just first, let's take a quick look at
what a posterior really looks like in practical terms. 

### Markov Chain Monte Carlo

#### What does a posterior look like? 

In Bayesian inference we do not get point estimates for the unknowns in our models; we estimate 
their posterior distributions. Ideally, we'd want to be able to make posterior inference by asking
questions like "what is the 5th percentile of the marginal posterior of $\theta_{1}$?". 
This would require that we can analytically derive these statistics from the posterior. Sadly, 
most posteriors do not have a closed form, and so we need to approximate. The two common types of 
approximation are: 

1. To approximate the posterior with a joint density for which we can analytically evaluate quantiles, expected values etc. 
This is the approach in Variational Bayes and penalized likelihood. 
2. To obtain many independent draws from the posterior, and evaluate quantiles, expected values of those draws. This is the approach 
in MCMC and ABC. 

In theory, our posterior $p(\theta | y)$ is an abstract distribution; in practice (when using MCMC), it's a matrix of data, where each column 
corresponds to a parameter, and each row a draw from $p(\theta | y)$. For instance, the first five rows of 
our posterior matrix for our linear model above might look like: 

```{r, echo = F, message = F, warning = F}
library(dplyr); library(knitr)

aa1 <- data_frame(a = rnorm(5, 2, .1), b = rnorm(5, .5, .05), sigma = rnorm(5, 1, .1)) 
aa1 %>%
  kable(digits = 2)

aa <- data_frame(a = rnorm(30, 2, .1), b = rnorm(30, .5, .05), sigma = rnorm(30, 1, .1)) 

```

Each of these parameter combinations implies a different predictive distribution. Consequently, they also imply
different predictive likelihoods for our out of sample datapoint $y_{j} = 1,x_{j} = 1$. This is illustrated
for 30 posterior replicates below. 

```{r, message = F, warning = F, echo = F}
library(ggplot2); library(dplyr)

aa <- data_frame(a = rnorm(30, 2, .1), b = rnorm(30, .5, .05), sigma = rnorm(30, 1, .1)) 
g <- ggplot(data.frame(x = c(-1, 5.5)), aes(x))
for(i in 1:nrow(aa)) {
 g <- g +
  stat_function(fun = dnorm, args = list(mean = c(aa$a[i] + aa$b[i]), sd = aa$sigma[i]), alpha = 0.2)  
}
 
g +
  geom_linerange(data = data.frame(ymin = 0, ymax = max(dnorm(1, aa$a + aa$b, aa$sigma)), x = 1), aes(x= x, ymin = ymin, ymax = ymax)) +
  geom_point(data= data.frame(x = rep(1, nrow(aa) +1), y = c(0, dnorm(1, aa$a + aa$b, aa$sigma))), aes(x = x, y = y), colour = "red") +
  xlab("y") +
  ylab("Density") +
  ggtitle("We can evaluate at the actual outcome\nover posterior draws", subtitle = "f(y | x, a, b, s)") +
  theme_bw()
```

The density of the data across posterior draws is known as the _log posterior density_ of
a datapoint. Unlike the likelihood, we are uncertain of its value and need to perform inference. To illustrate 
this, let's look at a histogram of the log posterior density in the above example (but with 500 replicates, not 30): 

```{r, message = F, warning = F, echo = F}

N <- 500
aa_big <- data_frame(a = rnorm(N, 2, .1), b = rnorm(N, .5, .05), sigma = rnorm(N, 1, .1)) 

data_frame(`Log density` = dnorm(1, aa_big$a + aa_big$b, aa_big$sigma, log = T)) %>% 
  ggplot(aes(x = `Log density`)) +
  geom_histogram() +
  ggtitle("Posterior density is a distribution, not a value") +
  theme_bw() #+
  # geom_vline(aes(xintercept = dnorm(1, 2.5, 1, log = T)), colour = "red") +
  # geom_vline(aes(xintercept = mean(`Log density`)), colour = "blue")

```

Note that the log posterior density can be evaluated _for each datapoint_; if we have many datapoints for which we want
to evaluate the LPD we simply need to evaluate it for each, and sum across all datapoints.



## Part 4: A Modern Statistical Workflow

Why use this workflow? Use when

  - You are building complex models and need to make sure that your software is correct
  - You want to learn about the model (and especially the impact of priors) more deeply

The workflow

**Prior predictive analysis**

- In order to choose priors; take our data
- Draw from the prior
- Draw from the distribution corresponding to the likelihood
- Are the outcomes at all plausible? 
  - Are they explosive? 
  - Do they assign probabilistic weight to impossible outcomes?
  - Do they imply economically implausible outcomes? Such as market shares of 1, or negative prices?
  - Do they interact with the scale of the input data?

- Plot your data and make notes of it
  - Outcome variables
  - Identify missing values
  - Correlations between explanators
  - Non-linearities
  - Interactions
  - Outliers
  - Scale

- Write down the likelihood
  - Choose based on conditional distribution of the data and theory
  
- Choose priors for the unknowns
  - We want priors that a) encode outside information, improve the geometry of the posterior
  - In practice this means choosing priors that look like normal(0, 1) and reparameterizing the model
  so that this makes sense
  
- Draw from the priors (with example of how to do this in Stan)
- Simulate fake data given your "knowns"
- Fit the model to the fake data
- Check the fit
  - Chains converged
  - Rhat close to 1?
  - Divergent transitions
  - Energy
  - Shinystan

- Reparameterize the model if you need to, and fit again
- Evaluate fit: check parameter estimates against those used to generate the data
  - Scatter plots of known vs estimates
  - Coverage
  - Order statistics

- Take to real data
  - Evaluate fit
  - Posterior predictive checking
  - Evaluate qualitative findings

- Discuss with a critical audience
- Make the model richer and repeat
- Apply decision theory to the estimate

```{r}

area_1 <- function(x) ifelse(x<= 0, 0, dnorm(x, 2, .9))

alpha <- 1
beta <- 5
delta <- 0.48
mm <- 20

truncated_dists <- ggplot(data.frame(x = c(-1,5))) +
  stat_function(aes(x = x), fun = dnorm, args = list(mean = 2, sd = 0.9), n = 1000) +
  stat_function(aes(x = x), fun = area_1, geom = "area", alpha = 0.2, n = 1000) +
  theme_hc() +
  labs(title = "Posterior distribution of a causal effect",
       x = "Effect size",
       y = "Density") +
  annotate("text", x = 2, y = 0.15, label = "Prob(effect > 0) = 0.987")




lossy <- function(x) ifelse((- beta*log(delta * x) - alpha)> mm | x<= 0, mm, -beta*log(delta * x) - alpha)
d_lossy <- function(x) (lossy(x) - lossy(x-0.0000000001))/0.0000000001
plot(d_lossy, -1, 5)
dnormloss <- function(x) dnorm(lossy(x), 2, .9)
plot(dnormloss, -1, 5)

d_lossy(4)
dloss <- function(x, mu, sigma) log(dnorm(lossy(x), mu, sigma)) + log(abs(d_lossy(x)))




loss_fn <- ggplot(data.frame(x = c(-1,5))) +
  stat_function(aes(x = x), fun = lossy, n = 1000) +
  theme_hc() +
  labs(title = "Loss function",
       x = "Effect size",
       y = "Loss") +
  annotate("text", x = 2, y = 10, label = "Fixed rollout cost less diminishing\n benefits of treatment")


loss_distribution <- ggplot(data.frame(x = c(-1,5))) +
  stat_function(aes(x = x), fun = function(x) dnorm(x, 2, .9)*ifelse((- beta*log(delta* x) - alpha)> mm | x<= 0, mm, -beta*log(delta * x) - alpha), n = 1000) +
  theme_hc() +
  labs(title = "Posterior density * loss",
       x = "Effect size",
       y = "Loss") +
  geom_vline(aes(xintercept = 2)) +
  annotate("text", x = 3, y = 0.2, label = "Need to integrate this function")


loss_dens <- ggplot(data.frame(x = c(-1,5))) +
  stat_function(aes(x = x), fun = dloss, args = list(mu = 2, sigma = 0.9), n = 1000) +
  theme_hc() +
  labs(title = "Posterior density * loss",
       x = "Effect size",
       y = "Loss") +
  geom_vline(aes(xintercept = 2))


x <- rnorm(1000, 2, .9)

mean(lossy(x))


gridExtra::grid.arrange(truncated_dists, loss_fn, loss_distribution)


```

